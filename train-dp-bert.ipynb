{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets==2.15.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-20T06:55:41.582280Z","iopub.execute_input":"2024-02-20T06:55:41.582525Z","iopub.status.idle":"2024-02-20T06:55:58.567645Z","shell.execute_reply.started":"2024-02-20T06:55:41.582503Z","shell.execute_reply":"2024-02-20T06:55:58.566416Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting datasets==2.15.0\n  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (1.24.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (11.0.0)\nCollecting pyarrow-hotfix (from datasets==2.15.0)\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.70.15)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0)\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.13.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.15.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.0) (1.16.0)\nDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.12.2\n    Uninstalling fsspec-2023.12.2:\n      Successfully uninstalled fsspec-2023.12.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.15.0 fsspec-2023.10.0 pyarrow-hotfix-0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, BertConfig\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom datasets import load_dataset\n\nimport torch\n\n# Initialize the model, tokenizer, and training settings\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\nconfig = BertConfig.from_pretrained(\"bert-base-multilingual-cased\")\nmodel = BertForSequenceClassification(config)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# -----------------------------------------------------------------------------\n# Load the dataset\ndataset = load_dataset(\"sepidmnorozy/Vietnamese_sentiment\")\n\n# Preprocess the data using the datasets library\ndef tokenize_and_encode(batch):\n    encoded = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n    return {\n        \"input_ids\": encoded[\"input_ids\"],\n        \"attention_mask\": encoded[\"attention_mask\"],\n        \"labels\": batch[\"label\"],\n    }\n\nencoded_train_dataset = dataset[\"train\"].map(tokenize_and_encode, batched=True, remove_columns=[\"text\"])\nencoded_eval_dataset = dataset[\"test\"].map(tokenize_and_encode, batched=True, remove_columns=[\"text\"])\nencoded_train_dataset.set_format(\"torch\")\nencoded_eval_dataset.set_format(\"torch\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:56:11.076774Z","iopub.execute_input":"2024-02-20T06:56:11.077108Z","iopub.status.idle":"2024-02-20T06:56:41.134574Z","shell.execute_reply.started":"2024-02-20T06:56:11.077083Z","shell.execute_reply":"2024-02-20T06:56:41.133696Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0238c06f28343f8b0055d991a56e8c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7908903d17c44bf1a515e2c8ab5e72fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efa2744f804541f588a15b14e52ab15a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04dfd3f065fe4d82b6044dfa13606a14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9631e4760734fd39e24e13ddd1a5259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/329k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88eadb373c4b42959ccd3bea288a9323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/52.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db9413afedb841e288569b21663c4baa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/99.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"909e212bc5cf47d9b2e07776293ab79b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc1545beaa14cf0ab793f17f9019bc4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:765: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dff988a19d7148a9a591156abc641671"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:765: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e038c432201b415a8fccc6fb3abde4e1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:765: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2384 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55541d05826c4cfdb9752ceace683975"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/685 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a184caef983b4b6d90266ebbe2b1753a"}},"metadata":{}}]},{"cell_type":"code","source":"# Create the DataLoaders\ntrain_dataloader = DataLoader(\n    encoded_train_dataset,\n    sampler=RandomSampler(encoded_train_dataset),\n    batch_size=8,\n    collate_fn=lambda x: {\n        \"input_ids\": torch.stack([sample[\"input_ids\"] for sample in x]),\n        \"attention_mask\": torch.stack([sample[\"attention_mask\"] for sample in x]),\n        \"labels\": torch.tensor([sample[\"labels\"] for sample in x]),\n    },\n)\n\neval_dataloader = DataLoader(\n    encoded_eval_dataset,\n    sampler=SequentialSampler(encoded_eval_dataset),\n    batch_size=8,\n    collate_fn=lambda x: {\n        \"input_ids\": torch.stack([sample[\"input_ids\"] for sample in x]),\n        \"attention_mask\": torch.stack([sample[\"attention_mask\"] for sample in x]),\n        \"labels\": torch.tensor([sample[\"labels\"] for sample in x]),\n    },\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:57:01.029259Z","iopub.execute_input":"2024-02-20T06:57:01.029600Z","iopub.status.idle":"2024-02-20T06:57:01.037505Z","shell.execute_reply.started":"2024-02-20T06:57:01.029574Z","shell.execute_reply":"2024-02-20T06:57:01.036607Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# Use DataParallel if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    print(f\"{torch.cuda.device_count()} GPUs are available. Using DataParallel.\")\n    model = torch.nn.DataParallel(model)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:57:04.521028Z","iopub.execute_input":"2024-02-20T06:57:04.521639Z","iopub.status.idle":"2024-02-20T06:57:04.971950Z","shell.execute_reply.started":"2024-02-20T06:57:04.521606Z","shell.execute_reply":"2024-02-20T06:57:04.971030Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"2 GPUs are available. Using DataParallel.\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(3):\n    # Training\n    model.train()\n    total_train_loss, total_train_correct = 0, 0\n    train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1} [Training]\", position=0, leave=True)\n    for batch in train_progress_bar:\n        input_ids, attention_masks, labels = (batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device))\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n        loss = criterion(outputs.logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n        preds = torch.argmax(outputs.logits, dim=1)\n        total_train_correct += (preds == labels).sum().item()\n    avg_train_loss = total_train_loss / len(train_dataloader)\n    avg_train_accuracy = total_train_correct / len(encoded_train_dataset)\n    print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss}, Train Accuracy: {avg_train_accuracy}\")\n\n    # Evaluation\n    model.eval()\n    total_eval_loss, total_eval_correct = 0, 0\n    eval_progress_bar = tqdm(eval_dataloader, desc=f\"Epoch {epoch + 1} [Evaluation]\", position=0, leave=True)\n    for batch in eval_progress_bar:\n        input_ids, attention_masks, labels = (batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device))\n\n        with torch.no_grad():\n            outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n        \n        loss = criterion(outputs.logits, labels)\n        total_eval_loss += loss.item()\n        preds = torch.argmax(outputs.logits, dim=1)\n        total_eval_correct += (preds == labels).sum().item()\n\n    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n    avg_eval_accuracy = total_eval_correct / len(encoded_eval_dataset)\n    print(f\"Epoch {epoch + 1}, Evaluation Loss: {avg_eval_loss}, Evaluation Accuracy: {avg_eval_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:57:14.392860Z","iopub.execute_input":"2024-02-20T06:57:14.393243Z","iopub.status.idle":"2024-02-20T07:02:01.530032Z","shell.execute_reply.started":"2024-02-20T06:57:14.393214Z","shell.execute_reply":"2024-02-20T07:02:01.529115Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Epoch 1 [Training]:   0%|          | 0/298 [00:00<?, ?it/s]2024-02-20 06:57:20.700867: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-20 06:57:20.700991: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-20 06:57:20.964838: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nEpoch 1 [Training]: 100%|██████████| 298/298 [01:39<00:00,  2.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Train Loss: 0.7113472109272976, Train Accuracy: 0.5113255033557047\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1 [Evaluation]: 100%|██████████| 86/86 [00:07<00:00, 10.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Evaluation Loss: 0.7065795292687971, Evaluation Accuracy: 0.5153284671532846\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 [Training]: 100%|██████████| 298/298 [01:21<00:00,  3.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Train Loss: 0.6488921793315234, Train Accuracy: 0.6111577181208053\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 [Evaluation]: 100%|██████████| 86/86 [00:08<00:00, 10.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Evaluation Loss: 0.5617056638002396, Evaluation Accuracy: 0.6934306569343066\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 [Training]: 100%|██████████| 298/298 [01:21<00:00,  3.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Train Loss: 0.5164058770509374, Train Accuracy: 0.7470637583892618\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 [Evaluation]: 100%|██████████| 86/86 [00:08<00:00, 10.45it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Evaluation Loss: 0.501136758299761, Evaluation Accuracy: 0.7547445255474453\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}